![Lint-free](https://github.com/nyu-software-engineering/containerized-app-exercise/actions/workflows/lint.yml/badge.svg)
![Machine Learning Client CI](https://github.com/software-students-spring2024/4-containerized-app-exercise-speedy/actions/workflows/machine_learning_client.yml/badge.svg)
![Web App CI](https://github.com/software-students-spring2024/4-containerized-app-exercise-speedy/actions/workflows/web-app.yml/badge.svg)

# Containerized App Exercise

Build a containerized app that uses machine learning. See [instructions](./instructions.md) for details.

## Description

The ASL interpreter app uses machine learning to recognize American Sign Language gestures from a live video feed, providing real-time translations into text output.
Designed for accessibility and ease of use, it offers a seamless, containerized experience that runs across three subsystems: gesture recognition, a web interface, and a database.

## Team

[Safia](https://github.com/safiabillah)

[Mel](https://github.com/melanie-y-zhang)

[Chloe](https://github.com/jh7316)

[Fatima Villena](https://github.com/favils)

## Instructions to Run

To do

## Task Board
[Task Board](https://github.com/orgs/software-students-fall2024/projects/119/views/1)
